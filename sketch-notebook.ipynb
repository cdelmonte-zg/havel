{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b6ac7ffd-01d8-49e7-9728-88753c893d43",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# pip install datasketch[redis]\n",
    "# pip install elasticsearch==7.10.0\n",
    "# pip install neo4j\n",
    "# pip install pyyaml\n",
    "# pip install deepdiff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "dc93d049-4578-483c-80b6-e5fd605d1596",
   "metadata": {},
   "outputs": [
    {
     "ename": "HTTPError",
     "evalue": "500 Server Error: Server Error for url: http://localhost:8998/sessions/36/statements",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-32-3990bab0be7c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;32mwith\u001b[0m \u001b[0mLivySession\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcreate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mLIVY_URL\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0msession\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m     \u001b[0msession\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'print(\"foo\")'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m     \u001b[0msession\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'files_df'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfiles_df\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\livy\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, code)\u001b[0m\n\u001b[0;32m    286\u001b[0m         \u001b[1;33m:\u001b[0m\u001b[0mparam\u001b[0m \u001b[0mcode\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mThe\u001b[0m \u001b[0mcode\u001b[0m \u001b[0mto\u001b[0m \u001b[0mrun\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    287\u001b[0m         \"\"\"\n\u001b[1;32m--> 288\u001b[1;33m         \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_execute\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    289\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mecho\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0moutput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    290\u001b[0m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\livy\\session.py\u001b[0m in \u001b[0;36m_execute\u001b[1;34m(self, code)\u001b[0m\n\u001b[0;32m    359\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    360\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_execute\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcode\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mOutput\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 361\u001b[1;33m         \u001b[0mstatement\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclient\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcreate_statement\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msession_id\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    362\u001b[0m         \u001b[0mintervals\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpolling_intervals\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0.1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0.2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0.3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0.5\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1.0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    363\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\livy\\client.py\u001b[0m in \u001b[0;36mcreate_statement\u001b[1;34m(self, session_id, code, kind)\u001b[0m\n\u001b[0;32m    288\u001b[0m             \u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"kind\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkind\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    289\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 290\u001b[1;33m         response = self._client.post(\n\u001b[0m\u001b[0;32m    291\u001b[0m             \u001b[1;34mf\"/sessions/{session_id}/statements\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    292\u001b[0m         )\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\livy\\client.py\u001b[0m in \u001b[0;36mpost\u001b[1;34m(self, endpoint, data)\u001b[0m\n\u001b[0;32m     71\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     72\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mpost\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mendpoint\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mdict\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mdict\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 73\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_request\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"POST\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mendpoint\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     74\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     75\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mdelete\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mendpoint\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mstr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"\"\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mdict\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\livy\\client.py\u001b[0m in \u001b[0;36m_request\u001b[1;34m(self, method, endpoint, data, params)\u001b[0m\n\u001b[0;32m     92\u001b[0m             \u001b[0mparams\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     93\u001b[0m         )\n\u001b[1;32m---> 94\u001b[1;33m         \u001b[0mresponse\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mraise_for_status\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     95\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mresponse\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjson\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     96\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\requests\\models.py\u001b[0m in \u001b[0;36mraise_for_status\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    941\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    942\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mhttp_error_msg\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 943\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mHTTPError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhttp_error_msg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    944\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    945\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mHTTPError\u001b[0m: 500 Server Error: Server Error for url: http://localhost:8998/sessions/36/statements"
     ]
    }
   ],
   "source": [
    "%%local\n",
    "\n",
    "from livy import LivySession\n",
    "from textwrap import dedent\n",
    "\n",
    "LIVY_URL = 'http://localhost:8998'\n",
    "\n",
    "with LivySession.create(LIVY_URL) as session:\n",
    "    session.run('print(\"foo\")')\n",
    "   \n",
    "    session.upload('files_df', files_df)\n",
    "\n",
    "    session.run(dedent(\"\"\"\n",
    "        print(files_df)\n",
    "    \"\"\"))\n",
    "    # top_ten_pandas.append(session.read('df'))\n",
    "            file = filename.first().getString(0)\n",
    "            df = spark.read.csv('/sample_data/')\n",
    "            top_ten = df.limit(10)\n",
    "top_ten_pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "90e532c6-c249-4cf5-be66-2c693158afbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\tLast name\tFirst name\tSSN\tTest1\tTest2\tTest3\tTest4\tFinal\tGrade\n",
      "\tAlfalfa\tAloysius\t123-45-6789\t40\t90\t100\t83\t49\tD-\n",
      "\tAlfred\tUniversity\t123-12-1234\t41\t97\t96\t97\t48\tD+\n",
      "\tGerty\tGramma\t567-89-0123\t41\t80\t60\t40\t44\tC\n",
      "\tAndroid\tElectric\t087-65-4321\t42\t23\t36\t45\t47\tB-\n",
      "\tBumpkin\tFred\t456-78-9012\t43\t78\t88\t77\t45\tA-\n",
      "\tRubble\tBetty\t234-56-7890\t44\t90\t80\t90\t46\tC-\n",
      "\tNoshow\tCecil\t345-67-8901\t45\t11\t-1\t4\t43\tF\n",
      "\tBuff\tBif\t632-79-9939\t46\t20\t30\t40\t50\tB+\n",
      "\tAirpump\tAndrew\t223-45-6789\t49\t1\t90\t100\t83\tA\n",
      "\tBackus\tJim\t143-12-1234\t48\t1\t97\t96\t97\tA+\n",
      "\tCarnivore\tArt\t565-89-0123\t44\t1\t80\t60\t40\tD+\n",
      "\tDandy\tJim\t087-75-4321\t47\t1\t23\t36\t45\tC+\n",
      "\tElephant\tIma\t456-71-9012\t45\t1\t78\t88\t77\tB-\n",
      "\tFranklin\tBenny\t234-56-2890\t50\t1\t90\t80\t90\tB-\n",
      "\tGeorge\tBoy\t345-67-3901\t40\t1\t11\t-1\t4\tB\n",
      "\tHeffalump\tHarvey\t632-79-9439\t30\t1\t20\t30\t40\tC\n",
      "\n",
      "\n",
      "_mapping response: {\n",
      "    \"X-TIKA:Parsed-By\": [\n",
      "        \"org.apache.tika.parser.DefaultParser\",\n",
      "        \"org.apache.tika.parser.csv.TextAndCSVParser\"\n",
      "    ],\n",
      "    \"X-TIKA:Parsed-By-Full-Set\": [\n",
      "        \"org.apache.tika.parser.DefaultParser\",\n",
      "        \"org.apache.tika.parser.csv.TextAndCSVParser\"\n",
      "    ],\n",
      "    \"X-TIKA:content_handler\": \"ToTextContentHandler\",\n",
      "    \"Content-Encoding\": \"ISO-8859-1\",\n",
      "    \"X-TIKA:parse_time_millis\": \"4\",\n",
      "    \"X-TIKA:embedded_depth\": \"0\",\n",
      "    \"resourceName\": \"b'webhdfs-v1-grades.csv'\",\n",
      "    \"csv:delimiter\": \"comma\",\n",
      "    \"Content-Length\": \"747\",\n",
      "    \"Content-Type\": \"text/csv; charset=ISO-8859-1; delimiter=comma\"\n",
      "} \n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%local\n",
    "\n",
    "# import parser object from tike\n",
    "from tika import parser  \n",
    "\n",
    "import tika\n",
    "  \n",
    "url = 'https://aclanthology.org/2020.acl-main.577.pdf'\n",
    "url = 'http://127.0.0.1:9864/webhdfs/v1/grades.csv?op=OPEN&namenoderpcaddress=master:9000&offset=0'\n",
    "pdfFile = parser.from_file(url)\n",
    "\n",
    "print(pdfFile['content'])\n",
    "\n",
    "print (\"_mapping response:\", json.dumps(pdfFile['metadata'], indent=4), \"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "87e87eca-bb6e-4dc8-9d8f-98d67a1b4a97",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adventureworks-for-postgres\n",
      "+--------------------+------+\n",
      "|            Filename|Length|\n",
      "+--------------------+------+\n",
      "|adventureworks-fo...|     0|\n",
      "+--------------------+------+"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%spark -o files_df\n",
    "\n",
    "import ntpath\n",
    "from pyspark.sql.types import ArrayType, StructField, StructType, StringType, IntegerType, DecimalType\n",
    "\n",
    "hadoop = sc._jvm.org.apache.hadoop\n",
    "\n",
    "fs = hadoop.fs.FileSystem\n",
    "conf = hadoop.conf.Configuration() \n",
    "path = hadoop.fs.Path('/sample_data/')\n",
    "\n",
    "files = []\n",
    "for f in fs.get(conf).listStatus(path):\n",
    "    filename = ntpath.basename(str(f.getPath()))\n",
    "    print(filename)\n",
    "    files.append((filename, f.getLen()))\n",
    "\n",
    "\n",
    "schema = StructType([\n",
    "    StructField('Filename', StringType(), True),\n",
    "    StructField('Length', StringType(), True)\n",
    "])\n",
    "\n",
    "# Convert list to RDD\n",
    "rdd = spark.sparkContext.parallelize(files)\n",
    "\n",
    "# Create data frame\n",
    "files_df = spark.createDataFrame(rdd,schema)\n",
    "files_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "97624a33-e541-4925-b4d6-3c5f1cb4fd10",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%local\n",
    "\n",
    "from datetime import datetime\n",
    "from datasketch import MinHash\n",
    "from datasketch import MinHashLSH\n",
    "from datasketch import LeanMinHash\n",
    "from elasticsearch import Elasticsearch\n",
    "from redis import StrictRedis\n",
    "import redis\n",
    "import json\n",
    "import pickle\n",
    "import base64\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import logging\n",
    "import yaml\n",
    "from yaml.loader import SafeLoader\n",
    "from dataclasses import dataclass\n",
    "from deepdiff import DeepDiff\n",
    "from pprint import pprint\n",
    "from typing import Dict, Any\n",
    "import hashlib\n",
    "\n",
    "logging.basicConfig(format='%(asctime)s - %(message)s', datefmt='%d-%b-%y %H:%M:%S')\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.ERROR)\n",
    "\n",
    "config = []\n",
    "config_data = []\n",
    "try:\n",
    "    with open('havel-config.yaml', 'r') as f:\n",
    "        config = yaml.load(f, Loader=SafeLoader)\n",
    "        for source in config['sources']:\n",
    "            for schema in config['sources'][source]:\n",
    "                url = config['sources'][source][schema]['url']\n",
    "                datasets = config['sources'][source][schema]['datasets']\n",
    "                for dataset in datasets:\n",
    "                    dataframe = 0\n",
    "                    config_data.append((source, schema, dataset, url, dataframe))\n",
    "\n",
    "except yaml.YAMLError:\n",
    "    print(\"Error in configuration file\")\n",
    "\n",
    "config_df = pd.DataFrame (config_data, columns = ['source', 'schema', 'dataset', 'url', 'dataframe'])\n",
    "date = '2022-11-20'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "93dbb6ed-2365-4ede-8eeb-3f1d5a8ebce7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>User</th><th>Current session?</th></tr><tr><td>46</td><td>application_1669190981900_0008</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://master:8088/proxy/application_1669190981900_0008/\">Link</a></td><td><a target=\"_blank\" href=\"http://worker1:8042/node/containerlogs/container_1669190981900_0008_01_000001/root\">Link</a></td><td>None</td><td>✔</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully passed 'config_df' as 'data' to Spark kernel"
     ]
    }
   ],
   "source": [
    "%%send_to_spark -i config_df -t df -n data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1cf3e862-9505-4402-9a15-4f6d2d7434b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully passed 'date' as 'date' to Spark kernel"
     ]
    }
   ],
   "source": [
    "%%send_to_spark -i date -t str -n date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dd69e99a-02e0-49f1-ba8b-28d47a1e6287",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%spark\n",
    "\n",
    "def read_data(i, date):\n",
    "    abs_path = 'sample_data'\n",
    "    source = data.rdd.map(lambda r: r.source).collect()[i]\n",
    "    schema = data.rdd.map(lambda r: r.schema).collect()[i]\n",
    "    dataset = data.rdd.map(lambda r: r.dataset).collect()[i]\n",
    "    globals()[f\"df_{i}\"] = spark.read.format(\"csv\").option(\"header\", \"true\").load(f\"/{abs_path}/{source}/{schema}/{date}/{dataset}.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "0f605ec4-9aa9-4f5a-9e1c-fa5a90fd262e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%spark -o df_0\n",
    "\n",
    "read_data(0, date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3e29a3f5-3068-402b-af3f-b45e5574014d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%spark -o df_1\n",
    "\n",
    "read_data(1, date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9e91831d-8e70-4de5-953f-37ca894b4d83",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%spark -o df_2\n",
    "\n",
    "read_data(2, date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d84d60f4-977a-45b6-84e9-c7bcaf711581",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%spark -o df_3\n",
    "\n",
    "read_data(3, date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "71fcce3c-d8df-4596-92d3-deb9be43f116",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%spark -o df_4\n",
    "\n",
    "read_data(4, date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b6d6db5d-0d79-4219-962d-304bceef6186",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%spark -o df_5\n",
    "\n",
    "read_data(5, date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e1553640-6217-407a-be71-f7954828c7dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%spark -o df_6\n",
    "\n",
    "read_data(6, date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bc511a88-a882-40be-be30-0661282e05fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%spark -o df_7\n",
    "\n",
    "read_data(7, date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6f719252-3d46-4459-96b6-3de272aaa82d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%spark -o df_8\n",
    "\n",
    "read_data(8, date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1ae207b4-d46f-492c-bacc-a8001731d533",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%spark -o df_9\n",
    "\n",
    "read_data(9, date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c84622dd-2e75-4cca-b50a-fe45f2154c05",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%spark -o df_10\n",
    "\n",
    "read_data(10, date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e13fd336-0d3d-4df8-8d25-550f682b550e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%spark -o df_11\n",
    "\n",
    "read_data(11, date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5b2a14c2-2088-4a33-92c1-0a8e98f530d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%spark -o df_12\n",
    "\n",
    "read_data(12, date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8bc1d47f-96a2-43ba-82a3-08c900c5c321",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%spark -o df_13\n",
    "\n",
    "read_data(13, date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "603c5593-5772-4fe4-a38e-f58ea312f0c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%spark -o df_14\n",
    "\n",
    "read_data(14, date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ea0f723c-7c39-4091-b120-ce738fa9a132",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%spark -o df_15\n",
    "\n",
    "read_data(15, date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2dd3d97c-da87-4ff0-b407-19caa58085a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%spark -o df_16\n",
    "\n",
    "df_16 = spark.read.format(\"csv\").option(\"header\", \"true\").load(\"/sample_data/adventureworks-for-postgres/person/2022-11-23/address.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "79c2d843-acdd-4a0e-a639-de0fc4f730bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2720ac46309048ad8ad82393dec750e0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HBox(children=(HTML(value='Type:'), Button(description='Table', layout=Layout(width='70px'), st…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2acb3b3037954d62a214033647fd3c89",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%local \n",
    "df_16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "394f993a-598b-4dd0-9921-0c6343bc9d00",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%local \n",
    "df_0 = df_16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "fbb6765d-8925-4009-bd55-b7eade095541",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%local\n",
    "\n",
    "def add_df_to_data(data, df, i):\n",
    "    row = list(data[i])\n",
    "    row[4] = df\n",
    "    data[i] = tuple(row)\n",
    "\n",
    "for i in range(0, len(config_data)):\n",
    "    add_df_to_data(config_data, globals()[f\"df_{i}\"], i)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "63518b48-8748-4113-a1d1-998bc0b39fcd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "88e0e22fefe445038693bfb7d666f12a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HBox(children=(HTML(value='Type:'), Button(description='Table', layout=Layout(width='70px'), st…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f3e4650bb9894c368fc2bd883b8127ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%local\n",
    "config_data[0][4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8f3c34f2-2849-4f06-806a-b6cf03390161",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%local\n",
    "\n",
    "\n",
    "def calc_idx_content(data):\n",
    "    return \"%s-%s-%s-%s\"%(data['source'].replace(' ', '_'), data['schema'].replace(' ', '_'), data['dataset'].replace(' ', '_'), data['attribute'].replace(' ', '_'))\n",
    "\n",
    "def calc_idx_signature(data):\n",
    "    return \"%s-%s-%s\"%(data['source'].replace(' ', '_'), data['schema'].replace(' ', '_'), data['dataset'].replace(' ', '_'))\n",
    "\n",
    "def get_df_data_types_as_dict(df):\n",
    "    return df.dtypes.apply(lambda x: x.name).to_dict()\n",
    "\n",
    "def serialize_dict_data_types(data_types):\n",
    "    return json.dumps(data_types, sort_keys=True)\n",
    "\n",
    "def deserialize_dict_data_types(ser_df_data_types):\n",
    "    return  json.loads(ser_df_data_types)\n",
    "\n",
    "def compare_dicts(dict_a, dict_b):\n",
    "    return DeepDiff(dict_a, dict_b)\n",
    "\n",
    "\n",
    "def dict_hash(dictionary: Dict[str, Any]) -> str:\n",
    "    \"\"\"MD5 hash of a dictionary.\"\"\"\n",
    "    dhash = hashlib.md5()\n",
    "    encoded = json.dumps(dictionary, sort_keys=True).encode()\n",
    "    dhash.update(encoded)\n",
    "    return dhash.hexdigest()\n",
    "\n",
    "\n",
    "def calc_profile_content(source, schema, dataset, df):\n",
    "    doc = []\n",
    "    min_hashes = []\n",
    "    \n",
    "    for c_n in df.columns:\n",
    "        m = MinHash(num_perm=128)\n",
    "        s = df[c_n]\n",
    "        dtype = str(s.dtype)\n",
    "        crd = s.nunique()\n",
    "        keywords = []\n",
    "        \n",
    "        for v in s:\n",
    "            if dtype == 'int64': \n",
    "                v = str(v)\n",
    "                m.update(v.encode('utf8'))\n",
    "            elif dtype == 'datetime64[ns]': \n",
    "                v = v.strftime(\"%Y-%m-%d %H:%M:%S.%f\")\n",
    "                m.update(v.encode('utf8'))\n",
    "            elif (isinstance(v, float)):\n",
    "                 m.update((str(v)).encode('utf8'))\n",
    "            else:\n",
    "                m.update(v.encode('utf8'))\n",
    "                keywords.append(v)\n",
    "        \n",
    "        data = {}\n",
    "        data['source'] = source\n",
    "        data['schema'] = schema\n",
    "        data['dataset'] = dataset\n",
    "        data['attribute'] = c_n\n",
    "        data['data_type'] = dtype\n",
    "        data['timestamp'] = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S.%f\")\n",
    "        data['size'] = s.size\n",
    "        data['cardinality'] = crd\n",
    "        data['uniqueness'] = crd / s.size     \n",
    "        data['min_hash'] =  m.hashvalues\n",
    "        data['min_hash_seed'] = m.seed\n",
    "        data['keywords'] = ' '.join(keywords)\n",
    "        \n",
    "        min_hashes.append((calc_idx_content(data), m))\n",
    "        doc.append(data)\n",
    "\n",
    "    return doc, min_hashes\n",
    "\n",
    "\n",
    "def calc_profile_signature(source, schema, dataset, df):\n",
    "    doc = []\n",
    "    min_hashes = []\n",
    "    keywords = []\n",
    "       \n",
    "    m = MinHash(num_perm=128)\n",
    "    for c in df.columns:\n",
    "        m.update(c.encode('utf8'))\n",
    "        keywords.append(c)\n",
    "        \n",
    "    data = {}\n",
    "    data['source'] = source\n",
    "    data['schema'] = schema\n",
    "    data['dataset'] = dataset\n",
    "    data['timestamp'] = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S.%f\")\n",
    "    data['min_hash'] =  m.hashvalues\n",
    "    data['min_hash_seed'] = m.seed\n",
    "    data['data_types'] = serialize_dict_data_types(get_df_data_types_as_dict(df))\n",
    "    data['keywords'] = ' '.join(keywords)\n",
    "    \n",
    "    min_hashes.append((calc_idx_signature(data), m))\n",
    "    \n",
    "    data['version'] = calc_version(calc_idx_signature(data), get_df_data_types_as_dict(df))\n",
    "    \n",
    "    doc.append(data)\n",
    "    \n",
    "    return doc, min_hashes\n",
    "\n",
    "\n",
    "\n",
    "def calc_version(k, d):\n",
    "    h = dict_hash(d)\n",
    "    v = version_cache.get(k)\n",
    "    version = 0\n",
    "    \n",
    "    print(v)\n",
    "    if v:\n",
    "        ser = json.loads(v)\n",
    "        print(ser)\n",
    "        print(k)\n",
    "        version = int(ser['version'])\n",
    "        if ser['hash'] != h:\n",
    "            version = version + 1\n",
    "            print('Version incremented for key: %s' % k)\\\n",
    "    \n",
    "    nd = {\n",
    "        'hash': h,\n",
    "        'version': version\n",
    "    }\n",
    "    \n",
    "    version_cache.set(k, json.dumps(nd))\n",
    "        \n",
    "    return version\n",
    "\n",
    "\n",
    "\n",
    "def insert_min_hashes(min_hashes, lsh_list):\n",
    "    for threshold, lsh in lsh_list.items():\n",
    "        with lsh.insertion_session() as session:\n",
    "            for key, minhash in min_hashes:\n",
    "                label = key\n",
    "                if debug: print(label)\n",
    "                if debug: print(minhash)\n",
    "                try:\n",
    "                    lsh.remove(label)\n",
    "                except:\n",
    "                    print(\"Key %s doesn't exist.\"%label) \n",
    "                session.insert(label, minhash)\n",
    "\n",
    "\n",
    "                \n",
    "def upsert(env, index, func_calc_idx, doc, v=True):\n",
    "    for data in doc:\n",
    "        res = es.index(index=index, id=func_calc_idx(data), body=data)\n",
    "\n",
    "    es.indices.refresh(index=index)\n",
    "    \n",
    "    if v is True:\n",
    "        res = es.search(size=max_size, index=index, body={\"query\": {\"match_all\": {}}})\n",
    "        if debug: print(\"Got %d Hits.\" % res['hits']['total']['value'])\n",
    "\n",
    "        \n",
    "def clear_cache_ns(ns):\n",
    "    \"\"\"\n",
    "    Clears a namespace in redis cache.\n",
    "    This may be very time consuming.\n",
    "    :param ns: str, namespace i.e your:prefix*\n",
    "    :return: int, num cleared keys\n",
    "    \"\"\"\n",
    "    count = 0\n",
    "    pipe = cache.pipeline()\n",
    "    for key in cache.scan_iter(ns):\n",
    "        pipe.delete(key)\n",
    "        count += 1\n",
    "    pipe.execute()\n",
    "    return count\n",
    "\n",
    "\n",
    "def clear_es(env):\n",
    "    # delete all\n",
    "    indices = [index_content[env], index_signature[env]]\n",
    "        \n",
    "    try:\n",
    "        es.delete_by_query(index=indices, body={\"query\": {\"match_all\": {}}})\n",
    "        es.indices.delete(index=indices, ignore=[400, 404])\n",
    "    except Exception as e:\n",
    "        print(\"Delete Index: %s\", e)\n",
    "\n",
    "def create_es_schema_signature(env):\n",
    "    schema = {\n",
    "        \"mappings\": {\n",
    "            \"properties\": {\n",
    "                \"source\": {\n",
    "                    \"type\": \"text\" # formerly \"string\"\n",
    "                },\n",
    "                \"schema\": {\n",
    "                    \"type\": \"text\" # formerly \"string\"\n",
    "                },\n",
    "                \"dataset\": {\n",
    "                    \"type\": \"text\" # formerly \"string\"\n",
    "                },\n",
    "                \"timestamp\": {\n",
    "                    \"type\": \"date\",\n",
    "                    \"format\": \"yyyy-MM-dd HH:mm:ss.SSSSSS\"\n",
    "                    # data format for Python's datetime.now() method\n",
    "                },\n",
    "                \"min_hash\": {\n",
    "                    \"ignore_malformed\": \"true\",\n",
    "                    \"type\": \"integer\"\n",
    "                },\n",
    "                \"min_hash_seed\": {\n",
    "                    \"type\": \"integer\"\n",
    "                },\n",
    "                \"data_types\": {\n",
    "                    \"type\": \"text\"\n",
    "                },\n",
    "                \"version\": {\n",
    "                    \"type\": \"integer\"\n",
    "                },\n",
    "                \"keywords\": {\n",
    "                    \"type\": \"text\"\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "\n",
    "    resp = es.indices.create(\n",
    "        index = index_signature[env],\n",
    "        body = schema\n",
    "    )\n",
    "    \n",
    "    if debug: print (\"_mapping response:\", json.dumps(resp, indent=4), \"\\n\")\n",
    "\n",
    "    \n",
    "def create_es_schema_content(env):\n",
    "    schema = {\n",
    "        \"mappings\": {\n",
    "            \"properties\": {\n",
    "                \"source\": {\n",
    "                    \"type\": \"text\" # formerly \"string\"\n",
    "                },\n",
    "                \"schema\": {\n",
    "                    \"type\": \"text\"\n",
    "                },\n",
    "                \"dataset\": {\n",
    "                    \"type\": \"text\"\n",
    "                },\n",
    "                \"attribute\": {\n",
    "                    \"type\": \"text\"\n",
    "                },\n",
    "                \"data_type\": {\n",
    "                    \"type\": \"text\"\n",
    "                },\n",
    "                 \"timestamp\": {\n",
    "                    \"type\": \"date\",\n",
    "                    \"format\": \"yyyy-MM-dd HH:mm:ss.SSSSSS\"\n",
    "                    # data format for Python's datetime.now() method\n",
    "                },\n",
    "                \"size\": {\n",
    "                    \"type\": \"integer\"\n",
    "                },\n",
    "                \"cardinality\": {\n",
    "                    \"type\": \"float\"\n",
    "                },\n",
    "                \"uniqueness\": {\n",
    "                    \"type\": \"float\"\n",
    "                },\n",
    "                \"min_hash\": {\n",
    "                    \"ignore_malformed\": \"true\",\n",
    "                    \"type\": \"integer\"\n",
    "                },\n",
    "                \"min_hash_seed\": {\n",
    "                    \"type\": \"integer\"\n",
    "                },\n",
    "                \"keywords\": {\n",
    "                    \"type\": \"text\"\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "\n",
    "    resp = es.indices.create(\n",
    "        index = index_content[env],\n",
    "        body = schema\n",
    "    )\n",
    "    \n",
    "    if debug: print (\"_mapping response:\", json.dumps(resp, indent=4), \"\\n\")\n",
    "\n",
    "    \n",
    "def create_profiles_content(source, schema, dataset, df, env, lsh_list):\n",
    "    doc, min_hashes = calc_profile_content(source, schema, dataset, df)\n",
    "    insert_min_hashes(min_hashes, lsh_list)\n",
    "    index = index_content[env]\n",
    "    \n",
    "    try:\n",
    "        upsert(env, index, calc_idx_content, doc)\n",
    "    except Exception as e:\n",
    "        print(\"something didn't work\", doc, \"\\n\", str(e))\n",
    "        \n",
    "\n",
    "def create_profiles_signature(source, schema, dataset, df, env, lsh_list):\n",
    "    doc, min_hashes = calc_profile_signature(source, schema, dataset, df)\n",
    "    insert_min_hashes(min_hashes, lsh_list)\n",
    "    index = index_signature[env]\n",
    "    \n",
    "    try:\n",
    "        upsert(env, index, calc_idx_signature, doc)\n",
    "    except Exception as e:\n",
    "        print(\"something didn't work\", doc, \"\\n\", str(e))\n",
    "\n",
    "\n",
    "def get_nodes_and_edges(env, index, lsh_list):\n",
    "    res = es.search(size=max_size, index=index, body={\"query\": {\"match_all\": {}}})\n",
    "    if debug: print(\"Got %d Hits.\" % res['hits']['total']['value'])\n",
    "\n",
    "    relations = []\n",
    "\n",
    "    for hit in res['hits']['hits']:\n",
    "        min_hash_hashes = np.array(hit['_source'][\"min_hash\"],  dtype=np.int64)\n",
    "        min_hash_seed = hit['_source'][\"min_hash_seed\"]\n",
    "\n",
    "        mt = MinHash(num_perm=128)\n",
    "        mt.hashvalues = min_hash_hashes\n",
    "        mt.seed = min_hash_seed\n",
    "        \n",
    "        for threshold, lsh in lsh_list.items():\n",
    "            bucket = lsh.query(mt)\n",
    "            relations.append((hit['_id'], bucket, str(threshold)))\n",
    "    return (res['hits']['hits'], relations)\n",
    "\n",
    "\n",
    "def search_dataset_for_topic(index, topic):\n",
    "    resp = es.search(\n",
    "        index= index,\n",
    "        body={\n",
    "            \"query\": {\n",
    "                \"match\": {\n",
    "                    \"keywords\": topic\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    res = []\n",
    "    for r in resp['hits']['hits']:\n",
    "        res.append({\n",
    "            'id': r['_id'], \n",
    "            'score': r['_score'],\n",
    "            'source': r['_source']['source'], \n",
    "            'schema': r['_source']['schema'], \n",
    "            'dataset': r['_source']['dataset'], \n",
    "            'attribute': r['_source']['attribute']})\n",
    "    \n",
    "    return res\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "5b0d75a4-1671-46ac-a5ef-77acce27d213",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%local\n",
    "\n",
    "from neo4j import GraphDatabase\n",
    "from textwrap import dedent\n",
    "\n",
    "class GraphStorage:\n",
    "\n",
    "    def __init__(self, uri, user, password, env, debug):\n",
    "        self.driver = GraphDatabase.driver(uri, auth=(user, password))\n",
    "        self.env = env\n",
    "        self.debug = debug\n",
    "        \n",
    "    def close(self):\n",
    "        self.driver.close()\n",
    "\n",
    "    def insert_column_node(self, idx_col, idx_tab, data):\n",
    "        with self.driver.session() as session:\n",
    "            res1 = session.execute_write(self._create_column_node, idx_col, data, self.env)\n",
    "            res2 = session.execute_write(self._create_column_table_relation, idx_col, idx_tab, self.env)\n",
    "            if self.debug: print(res1)\n",
    "            if self.debug: print(res2)\n",
    "    \n",
    "    def insert_table_node(self, idx, data):\n",
    "        with self.driver.session() as session:\n",
    "            res = session.execute_write(self._create_table_node, idx, data, self.env)\n",
    "            if self.debug: print(res)\n",
    "            \n",
    "    def add_column_column_relation(self, idx_a, idx_b, threshold):\n",
    "        with self.driver.session() as session:\n",
    "            res = session.execute_write(self._create_column_column_relation, idx_a, idx_b, threshold, self.env)\n",
    "            if self.debug: print(res)\n",
    "    \n",
    "    def add_table_table_relation(self, idx_a, idx_b, threshold):\n",
    "        with self.driver.session() as session:\n",
    "            res = session.execute_write(self._create_table_table_relation, idx_a, idx_b, threshold, self.env)\n",
    "            if self.debug: print(res)\n",
    "            \n",
    "    def remove_self_relations(self):\n",
    "        with self.driver.session() as session:\n",
    "            res = session.execute_write(self._delete_self_relations, self.env)\n",
    "            if self.debug: print(res)     \n",
    "    \n",
    "    def delete_all(self):\n",
    "        with self.driver.session() as session:\n",
    "            res = session.execute_write(self._delete_all, self.env)\n",
    "            if self.debug: print(res)\n",
    "    \n",
    "    \n",
    "    def find_related_tables_by_signature_to_2_grade(self, dataset, weight, dastart, daend):\n",
    "        with self.driver.session() as session:\n",
    "            res = session.execute_read(self._fetch_related_tables_by_signature_to_2_grade, dataset, weight, dastart, daend, self.env)\n",
    "            if self.debug: print(res)\n",
    "\n",
    "            return res\n",
    "        \n",
    "    def find_related_tables_by_content_to_1_grad(self, attribute, weight, dastart, daend):\n",
    "        with self.driver.session() as session:\n",
    "            res = session.execute_read(self._fetch_related_tables_by_content_to_1_grade, attribute, weight, dastart, daend, self.env)\n",
    "            if self.debug: print(res)\n",
    "\n",
    "            return res    \n",
    "    \n",
    "    \n",
    "    @staticmethod\n",
    "    def _create_column_node(tx, idx, data, env):\n",
    "        result = tx.run(\"MERGE (a:Column {idx: $idx, source: $source, schema: $schema, dataset: $dataset, version: $version, env: $env, \"\n",
    "                        \"attribute: $attribute, data_type: $data_type, cardinality: $cardinality, uniqueness: $uniqueness}) \"\n",
    "                        \"ON CREATE SET a.createdAt = timestamp() \"\n",
    "                        \"ON MATCH SET a.updatetAt = timestamp() \"\n",
    "                        \"RETURN a.name + ' created as node ' + id(a) \",\n",
    "                        source=data['source'], schema=data['schema'], dataset=data['dataset'], version=1, timestamp=data['timestamp'],\n",
    "                        attribute=data['attribute'],  data_type=data['data_type'], cardinality=data['cardinality'],  uniqueness=data['uniqueness'], \n",
    "                        env=env, idx=idx)\n",
    "        \n",
    "        return result.single()[0]\n",
    "    \n",
    "    \n",
    "    @staticmethod\n",
    "    def _create_table_node(tx, idx, data, env):\n",
    "#         q = '''\n",
    "#             MATCH (a:Table{idx: $idx, source: $source, schema: $schema, dataset: $dataset, version: $version, env: $env, timestamp: $timestamp})\n",
    "#             WITH a\n",
    "#             FOREACH(ignoreMe IN CASE WHEN n.version < $new_version THEN [1] ELSE [] END | \n",
    "#                 MERGE (a:Table{idx: $idx, source: $source, schema: $schema, dataset: $dataset, version: $version, env: $env, timestamp: $timestamp})\n",
    "#                 ON CREATE SET a.createdAt = timestamp()\n",
    "#                 ON MATCH SET a.updatetAt = timestamp()\n",
    "#             )\n",
    "#             FOREACH(ignoreMe IN CASE WHEN NOT n.version < $new_version THEN [1] ELSE [] END | \n",
    "#                 MATCH (a:Table{idx: $idx, source: $source, schema: $schema, dataset: $dataset, version: $version, env: $env, timestamp: $timestamp})\n",
    "#                 SET a.createdAt = timestamp()\n",
    "#                 SET a.updatetAt = timestamp()\n",
    "#             )\n",
    "#             RETURN a.name + ' created as node ' + id(a)\n",
    "#         '''\n",
    "        \n",
    "        \n",
    "        \n",
    "#             MATCH (a:Table{idx: 'adventureworks-for-postgres-person-person-lastname', source: 'adventureworks-for-postgres', \n",
    "#                         schema: 'person', dataset: 'person', version: 1, env: 'dev', timestamp: timestamp()})\n",
    "#             WITH a\n",
    "#             FOREACH(ignoreMe IN CASE WHEN a.version < 2 THEN [1] ELSE [] END | \n",
    "#                 MERGE (a:Table{idx: 'adventureworks-for-postgres-person-person-lastname', source: 'adventureworks-for-postgres', \n",
    "#                         schema: 'person', dataset: 'person', version: 1, env: 'dev', timestamp: timestamp()})\n",
    "#                 ON CREATE SET a.createdAt = timestamp()\n",
    "#                 ON MATCH SET a.updatetAt = timestamp()\n",
    "#             )\n",
    "\n",
    "#             RETURN a.name + ' created as node ' + id(a)\n",
    "        \n",
    "        \n",
    "        \n",
    "#         [{'id': 'adventureworks-for-postgres-person-person-lastname', 'score': 5.1225724, 'source': 'adventureworks-for-postgres', 'schema': 'person', 'dataset': 'person',\n",
    "#           'attribute': 'lastname'}, {'id': 'adventureworks-for-postgres-person-vadditionalcontactinfo-lastname', 'score': 5.1225724, 'source': 'adventureworks-for-postgres', 'schema': 'person', 'dataset': 'vadditionalcontactinfo', 'attribute': 'lastname'}]\n",
    "        \n",
    "        \n",
    "#         MATCH (n:Table{dataset: 'address'})\n",
    "# WITH n\n",
    "# FOREACH(ignoreMe IN CASE WHEN n.version=2 THEN [1] ELSE [] END | \n",
    "#     MERGE(n {dataset: 'address'})\n",
    "#     ON MATCH SET n.updatetAt = timestamp()\n",
    "# )\n",
    "# RETURN n\n",
    "        \n",
    "        \n",
    "        \n",
    "        result = tx.run(\"MERGE (a:Table{idx: $idx, source: $source, schema: $schema, dataset: $dataset, version: $version, env: $env}) \"\n",
    "                        \"ON CREATE SET a.createdAt = timestamp() \"\n",
    "                        \"ON MATCH SET a.updatetAt = timestamp() \"\n",
    "                        \"RETURN a.name + ' created as node ' + id(a) \", \n",
    "                        source=data['source'], schema=data['schema'], dataset=data['dataset'], version=data['version'], timestamp=data['timestamp'], \n",
    "                        env=env, idx=idx)\n",
    "        return result.single()[0]\n",
    "    \n",
    "    \n",
    "    @staticmethod\n",
    "    def _create_table_table_relation(tx, idx_a, idx_b, weight, env):\n",
    "        result = tx.run(\"MATCH (a:Table {idx: $idx_a, env: $env}), (b:Table {idx: $idx_b, env: $env}) \"\n",
    "                        \"MERGE (a)-[r:IS_SIMILAR_TO {weight: $weight}]-(b) \"\n",
    "                        \"ON CREATE SET r.createdAt = timestamp() , r.updatetAt = timestamp()\"\n",
    "                        \"ON MATCH SET r.updatetAt = timestamp() \"\n",
    "                        \"RETURN 'relation ' + type(r) + ' created between ' + a.dataset + ' and ' + b.dataset\",\n",
    "                        idx_a=idx_a, idx_b=idx_b, weight=weight, env=env)\n",
    "        \n",
    "        entire_result = []\n",
    "        for record in result:\n",
    "            entire_result.append(record[0])\n",
    "            \n",
    "        return entire_result\n",
    "    \n",
    "    @staticmethod\n",
    "    def _create_column_table_relation(tx, idx_col, idx_tab, env):\n",
    "        result = tx.run(\"MATCH (t:Table {idx: $idx_tab, env: $env}) \"\n",
    "                        \"WITH max(t.version) AS maximum \"\n",
    "                        \"MATCH (c:Column {idx: $idx_col, env: $env}), (t:Table {idx: $idx_tab, env: $env}) \"\n",
    "                        \"WHERE t.version = maximum \"\n",
    "                        \"MERGE (c)-[r:IS_ATTRIBUTE_OF {weight: '1.0'}]->(t) \"\n",
    "                        \"ON CREATE SET r.createdAt = timestamp(), r.updatetAt = timestamp() \"\n",
    "                        \"ON MATCH SET r.updatetAt = timestamp() \"\n",
    "                        \"RETURN 'relation ' + type(r) + ' created between ' + c.attribute + ' and ' + t.dataset\", \n",
    "                        idx_tab=idx_tab, idx_col=idx_col, env=env)\n",
    "        \n",
    "        entire_result = []\n",
    "        for record in result:\n",
    "            entire_result.append(record[0])\n",
    "            \n",
    "        return entire_result\n",
    "\n",
    "    @staticmethod\n",
    "    def _create_column_column_relation(tx, idx_a, idx_b, weight, env):\n",
    "        result = tx.run(\"MATCH (a:Column {idx: $idx_a, env: $env}), (b:Column {idx: $idx_b, env: $env}) \"\n",
    "                        \"MERGE (a)-[r:IS_SIMILAR_TO {weight: $weight}]-(b) \"\n",
    "                        \"ON CREATE SET r.createdAt = timestamp(), r.updatetAt = timestamp() \"\n",
    "                        \"ON MATCH SET r.updatetAt = timestamp() \"\n",
    "                        \"RETURN 'relation ' + type(r) + ' created between ' + a.attribute + ' and ' + b.attribute\", \n",
    "                        idx_a=idx_a, idx_b=idx_b, env=env, weight=weight)\n",
    "        \n",
    "        entire_result = []\n",
    "        for record in result:\n",
    "            entire_result.append(record[0])\n",
    "            \n",
    "        return entire_result\n",
    "    \n",
    "    \n",
    "    @staticmethod\n",
    "    def _delete_self_relations(tx, env):\n",
    "        result = tx.run(\"MATCH (a {env: $env})-[rel:IS_SIMILAR_TO]->(a) DELETE rel\", env=env)\n",
    "        \n",
    "        entire_result = []\n",
    "        for record in result:\n",
    "            entire_result.append(record[0])\n",
    "            \n",
    "        return entire_result\n",
    "   \n",
    "    @staticmethod\n",
    "    def _delete_all(tx, env):\n",
    "        result = tx.run(\"MATCH (n {env: $env}) DETACH DELETE n\", env=env)\n",
    "        \n",
    "        entire_result = []\n",
    "        for record in result:\n",
    "            entire_result.append(record[0])\n",
    "            \n",
    "        return entire_result\n",
    "    \n",
    "    @staticmethod\n",
    "    def _fetch_related_tables_by_signature_to_2_grade(tx, dataset, weight, dastart, daend, env):\n",
    "        q = '''\n",
    "            MATCH (a:Table)-[r1:IS_SIMILAR_TO]-(b:Table) \n",
    "            WHERE a.dataset = $dataset AND a.env = $env AND b.env = $env AND r1.weight = $weight\n",
    "                AND $dastart <= r1.updatetAt AND r1.updatetAt <= $daend\n",
    "            OPTIONAL MATCH (b:Table)-[r2:IS_SIMILAR_TO]-(c:Table) \n",
    "            WHERE c.env = $env AND c <> a AND r2.weight = $weight \n",
    "                AND $dastart <= r2.updatetAt AND r2.updatetAt <= $daend\n",
    "            RETURN a, b, c\n",
    "        '''\n",
    "        \n",
    "        result = tx.run(dedent(q), env=env, dataset=dataset, weight=weight, dastart=dastart, daend=daend)\n",
    "\n",
    "        entire_result = []\n",
    "        for record in result:\n",
    "            entire_result.append((record[0], record[1], record[2]))\n",
    "\n",
    "        return entire_result\n",
    "    \n",
    "    @staticmethod\n",
    "    def _fetch_related_tables_by_content_to_1_grade(tx, attribute, weight, dastart, daend, env):\n",
    "        q = '''\n",
    "            MATCH (a:Column)-[r1:IS_ATTRIBUTE_OF]-(b:Table)<-[r2:IS_ATTRIBUTE_OF]-(c:Column)-[r3:IS_SIMILAR_TO]-(d:Column)-[r4:IS_ATTRIBUTE_OF]->(e:Table)\n",
    "            WHERE a.attribute = $attribute AND a.env=$env AND b.env=$env AND c.env=$env AND d.env=$env AND e.env=$env AND b <> e AND r3.weight=$weight\n",
    "                AND $dastart <= r1.updatetAt AND r1.updatetAt <= $daend\n",
    "                AND $dastart <= r2.updatetAt AND r2.updatetAt <= $daend\n",
    "                AND $dastart <= r3.updatetAt AND r3.updatetAt <= $daend\n",
    "                AND $dastart <= r4.updatetAt AND r4.updatetAt <= $daend\n",
    "            RETURN b, c, d, e\n",
    "        '''\n",
    "        \n",
    "        result = tx.run(dedent(q), env=env, attribute=attribute, weight=weight, dastart=dastart, daend=daend)\n",
    "\n",
    "        entire_result = []\n",
    "        for record in result:\n",
    "            entire_result.append((record[0], record[1], record[2], record[3]))\n",
    "\n",
    "        return entire_result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "af62c126-f551-47d6-a15d-74c9cf704169",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'{\"hash\": \"13938f2e111f6406855874d0cfd0f286\", \"version\": 4}'\n",
      "{'hash': '13938f2e111f6406855874d0cfd0f286', 'version': 4}\n",
      "adventureworks-for-postgres-person-address\n",
      "Version incremented for key: adventureworks-for-postgres-person-address\n",
      "b'{\"hash\": \"d3ceff9c1ba2f60621c24fd72bd6629c\", \"version\": 1}'\n",
      "{'hash': 'd3ceff9c1ba2f60621c24fd72bd6629c', 'version': 1}\n",
      "adventureworks-for-postgres-person-addresstype\n",
      "b'{\"hash\": \"e08d5d9a9c77d127c4f6ca62a693bce8\", \"version\": 1}'\n",
      "{'hash': 'e08d5d9a9c77d127c4f6ca62a693bce8', 'version': 1}\n",
      "adventureworks-for-postgres-person-businessentity\n",
      "b'{\"hash\": \"18da5d35d8cc9638ffca9f5328607351\", \"version\": 1}'\n",
      "{'hash': '18da5d35d8cc9638ffca9f5328607351', 'version': 1}\n",
      "adventureworks-for-postgres-person-businessentityaddress\n",
      "b'{\"hash\": \"50b8bf6d8581675def8ba6ef96837c9e\", \"version\": 1}'\n",
      "{'hash': '50b8bf6d8581675def8ba6ef96837c9e', 'version': 1}\n",
      "adventureworks-for-postgres-person-businessentitycontact\n",
      "b'{\"hash\": \"45feb6c7d21b8d4cd11189a75f7df563\", \"version\": 1}'\n",
      "{'hash': '45feb6c7d21b8d4cd11189a75f7df563', 'version': 1}\n",
      "adventureworks-for-postgres-person-contacttype\n",
      "b'{\"hash\": \"45feb6c7d21b8d4cd11189a75f7df563\", \"version\": 1}'\n",
      "{'hash': '45feb6c7d21b8d4cd11189a75f7df563', 'version': 1}\n",
      "adventureworks-for-postgres-person-contacttype\n",
      "b'{\"hash\": \"5dfd0065fae19cd804806c0b84dce347\", \"version\": 1}'\n",
      "{'hash': '5dfd0065fae19cd804806c0b84dce347', 'version': 1}\n",
      "adventureworks-for-postgres-person-countryregion\n",
      "b'{\"hash\": \"26435fc737892b1d3316b9b1dccb5fc7\", \"version\": 1}'\n",
      "{'hash': '26435fc737892b1d3316b9b1dccb5fc7', 'version': 1}\n",
      "adventureworks-for-postgres-person-emailaddress\n",
      "b'{\"hash\": \"d9fa15b1db33d89d8712241999140d5f\", \"version\": 1}'\n",
      "{'hash': 'd9fa15b1db33d89d8712241999140d5f', 'version': 1}\n",
      "adventureworks-for-postgres-person-password\n",
      "b'{\"hash\": \"0b64b48d93f51c8c311d41f7aab526e8\", \"version\": 1}'\n",
      "{'hash': '0b64b48d93f51c8c311d41f7aab526e8', 'version': 1}\n",
      "adventureworks-for-postgres-person-person\n",
      "b'{\"hash\": \"3f20b0690a9d3e6376add512e6629060\", \"version\": 1}'\n",
      "{'hash': '3f20b0690a9d3e6376add512e6629060', 'version': 1}\n",
      "adventureworks-for-postgres-person-personphone\n",
      "b'{\"hash\": \"03ed566a525f0c08f6fbd906dab5dd85\", \"version\": 1}'\n",
      "{'hash': '03ed566a525f0c08f6fbd906dab5dd85', 'version': 1}\n",
      "adventureworks-for-postgres-person-phonenumbertype\n",
      "b'{\"hash\": \"58f4b5c3d82a59e9d7bd1814a3e6c312\", \"version\": 1}'\n",
      "{'hash': '58f4b5c3d82a59e9d7bd1814a3e6c312', 'version': 1}\n",
      "adventureworks-for-postgres-person-stateprovince\n",
      "b'{\"hash\": \"8da284cf055ee14c1db0c44966f7ed7e\", \"version\": 1}'\n",
      "{'hash': '8da284cf055ee14c1db0c44966f7ed7e', 'version': 1}\n",
      "adventureworks-for-postgres-person-vadditionalcontactinfo\n",
      "b'{\"hash\": \"778cf3734337f29c14b27816f0a7b7de\", \"version\": 1}'\n",
      "{'hash': '778cf3734337f29c14b27816f0a7b7de', 'version': 1}\n",
      "adventureworks-for-postgres-person-address_2\n"
     ]
    }
   ],
   "source": [
    "%%local\n",
    "\n",
    "env = 'dev'\n",
    "reset_on_restart = False\n",
    "debug = False\n",
    "\n",
    "cache = StrictRedis()\n",
    "es = Elasticsearch()\n",
    "graph = GraphStorage(\"bolt://localhost:7687\", \"neo4j\", \"password\", env, debug)\n",
    "\n",
    "pool = redis.ConnectionPool(host='localhost', port=6379, db=4)\n",
    "version_cache = redis.Redis(connection_pool=pool)\n",
    "\n",
    "index_content = {\n",
    "    'dev': 'dev-havel-index-content',\n",
    "    'test': 'test-havel-index-content',\n",
    "    'prod': 'prod-havel-index-content'\n",
    "}\n",
    "\n",
    "index_signature = {\n",
    "    'dev': 'dev-havel-index-signature',\n",
    "    'test': 'test-havel-index-signature',\n",
    "    'prod': 'prod-havel-index-signature'\n",
    "}\n",
    "\n",
    "max_size = '10000'\n",
    "\n",
    "if (reset_on_restart):\n",
    "    clear_cache_ns('db0')\n",
    "    clear_cache_ns('db1')\n",
    "    clear_cache_ns('db2')\n",
    "    clear_cache_ns('db3')\n",
    "    \n",
    "    clear_es(env)\n",
    "    create_es_schema_content(env)\n",
    "    create_es_schema_signature(env)\n",
    "    \n",
    "    graph.delete_all()   \n",
    "\n",
    "\n",
    "lsh = {\n",
    "    '0.5':  MinHashLSH(\n",
    "        threshold=0.5, num_perm=128, storage_config={\n",
    "        'type': 'redis',\n",
    "        'basename': bytearray(index_content[env], 'utf-8'),\n",
    "        'redis': {'host': 'localhost', 'port': 6379, 'db': 0}\n",
    "    }),\n",
    "    '0.7': MinHashLSH(\n",
    "        threshold=0.7, num_perm=128, storage_config={\n",
    "        'type': 'redis',\n",
    "        'basename': bytearray(index_content[env], 'utf-8'),\n",
    "        'redis': {'host': 'localhost', 'port': 6379, 'db': 1}\n",
    "    }),\n",
    "    '0.9': MinHashLSH(\n",
    "        threshold=0.9, num_perm=128, storage_config={\n",
    "        'type': 'redis',\n",
    "        'basename': bytearray(index_content[env], 'utf-8'),\n",
    "        'redis': {'host': 'localhost', 'port': 6379, 'db': 2}\n",
    "     })\n",
    "}\n",
    "\n",
    "lsh_sig = {\n",
    "    '0.8': MinHashLSH(\n",
    "        threshold=0.8, num_perm=128, storage_config={\n",
    "        'type': 'redis',\n",
    "        'basename': bytearray(index_content[env], 'utf-8'),\n",
    "        'redis': {'host': 'localhost', 'port': 6379, 'db': 3}\n",
    "     })\n",
    "}\n",
    "\n",
    "\n",
    "# Create Profiles\n",
    "for source, schema, dataset, url, df in config_data: create_profiles_signature(source, schema, dataset, df, env, lsh_sig)\n",
    "for source, schema, dataset, url, df in config_data: create_profiles_content(source, schema, dataset, df, env, lsh)\n",
    "\n",
    "# Create Table nodes\n",
    "nodes, edges = get_nodes_and_edges(env, index_signature[env], lsh_sig)\n",
    "for hit in nodes: graph.insert_table_node(hit['_id'], hit['_source'])\n",
    "for k,b,th in edges:\n",
    "    for v in b: \n",
    "        if debug: print(\"key: %s - bucket: %s - threshold: %s\"%(k, b, th))\n",
    "        graph.add_table_table_relation(k,v,th)\n",
    "\n",
    "# Create Column nodes\n",
    "nodes, edges = get_nodes_and_edges(env, index_content[env], lsh)\n",
    "for hit in nodes: graph.insert_column_node(hit['_id'], calc_idx_signature(hit['_source']), hit['_source'])\n",
    "for k,b,th in edges:\n",
    "    for v in b: \n",
    "        if debug: print(\"key: %s - bucket: %s - threshold: %s\"%(k, b, th))\n",
    "        graph.add_column_column_relation(k,v,th)\n",
    "\n",
    "# Cleanup\n",
    "graph.remove_self_relations()\n",
    "\n",
    "graph.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "8cc3dfbd-a4ab-4fdb-b922-b9677a516fcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1669201932000\n",
      "1669223706241\n",
      "[{'id': 'adventureworks-for-postgres-person-vadditionalcontactinfo-lastname', 'score': 5.1225724, 'source': 'adventureworks-for-postgres', 'schema': 'person', 'dataset': 'vadditionalcontactinfo', 'attribute': 'lastname'}, {'id': 'adventureworks-for-postgres-person-person-lastname', 'score': 5.1225724, 'source': 'adventureworks-for-postgres', 'schema': 'person', 'dataset': 'person', 'attribute': 'lastname'}]\n",
      "[]\n",
      "[]\n",
      "person -> (rowguid - rowguid) <- vadditionalcontactinfo\n",
      "\n",
      "person -> (emailpromotion - phonenumbertypeid) <- personphone\n",
      "\n",
      "person -> (emailpromotion - phonenumbertypeid) <- phonenumbertype\n",
      "\n",
      "person -> (middlename - middlename) <- vadditionalcontactinfo\n",
      "\n",
      "person -> (firstname - firstname) <- vadditionalcontactinfo\n",
      "\n",
      "person -> (businessentityid - businessentityid) <- personphone\n",
      "\n",
      "person -> (businessentityid - businessentityid) <- vadditionalcontactinfo\n",
      "\n",
      "person -> (businessentityid - businessentityid) <- password\n",
      "\n",
      "person -> (businessentityid - personid) <- businessentitycontact\n",
      "\n",
      "person -> (businessentityid - businessentityid) <- businessentity\n",
      "\n",
      "person -> (businessentityid - emailaddressid) <- emailaddress\n",
      "\n",
      "person -> (businessentityid - businessentityid) <- emailaddress\n",
      "\n",
      "vadditionalcontactinfo -> (modifieddate - modifieddate) <- address_2\n",
      "\n",
      "vadditionalcontactinfo -> (modifieddate - modifieddate) <- personphone\n",
      "\n",
      "vadditionalcontactinfo -> (modifieddate - modifieddate) <- password\n",
      "\n",
      "vadditionalcontactinfo -> (modifieddate - modifieddate) <- businessentityaddress\n",
      "\n",
      "vadditionalcontactinfo -> (modifieddate - modifieddate) <- emailaddress\n",
      "\n",
      "vadditionalcontactinfo -> (modifieddate - modifieddate) <- address\n",
      "\n",
      "vadditionalcontactinfo -> (rowguid - rowguid) <- person\n",
      "\n",
      "vadditionalcontactinfo -> (middlename - middlename) <- person\n",
      "\n",
      "vadditionalcontactinfo -> (firstname - firstname) <- person\n",
      "\n",
      "vadditionalcontactinfo -> (businessentityid - businessentityid) <- personphone\n",
      "\n",
      "vadditionalcontactinfo -> (businessentityid - businessentityid) <- person\n",
      "\n",
      "vadditionalcontactinfo -> (businessentityid - businessentityid) <- password\n",
      "\n",
      "vadditionalcontactinfo -> (businessentityid - personid) <- businessentitycontact\n",
      "\n",
      "vadditionalcontactinfo -> (businessentityid - businessentityid) <- businessentity\n",
      "\n",
      "vadditionalcontactinfo -> (businessentityid - emailaddressid) <- emailaddress\n",
      "\n",
      "vadditionalcontactinfo -> (businessentityid - businessentityid) <- emailaddress\n",
      "\n",
      "person -> (rowguid - rowguid) <- vadditionalcontactinfo\n",
      "\n",
      "person -> (emailpromotion - phonenumbertypeid) <- personphone\n",
      "\n",
      "person -> (emailpromotion - phonenumbertypeid) <- phonenumbertype\n",
      "\n",
      "person -> (middlename - middlename) <- vadditionalcontactinfo\n",
      "\n",
      "person -> (firstname - firstname) <- vadditionalcontactinfo\n",
      "\n",
      "person -> (businessentityid - businessentityid) <- personphone\n",
      "\n",
      "person -> (businessentityid - businessentityid) <- vadditionalcontactinfo\n",
      "\n",
      "person -> (businessentityid - businessentityid) <- password\n",
      "\n",
      "person -> (businessentityid - personid) <- businessentitycontact\n",
      "\n",
      "person -> (businessentityid - businessentityid) <- businessentity\n",
      "\n",
      "person -> (businessentityid - emailaddressid) <- emailaddress\n",
      "\n",
      "person -> (businessentityid - businessentityid) <- emailaddress\n",
      "\n",
      "vadditionalcontactinfo -> (modifieddate - modifieddate) <- address_2\n",
      "\n",
      "vadditionalcontactinfo -> (modifieddate - modifieddate) <- personphone\n",
      "\n",
      "vadditionalcontactinfo -> (modifieddate - modifieddate) <- password\n",
      "\n",
      "vadditionalcontactinfo -> (modifieddate - modifieddate) <- businessentityaddress\n",
      "\n",
      "vadditionalcontactinfo -> (modifieddate - modifieddate) <- emailaddress\n",
      "\n",
      "vadditionalcontactinfo -> (modifieddate - modifieddate) <- address\n",
      "\n",
      "vadditionalcontactinfo -> (rowguid - rowguid) <- person\n",
      "\n",
      "vadditionalcontactinfo -> (middlename - middlename) <- person\n",
      "\n",
      "vadditionalcontactinfo -> (firstname - firstname) <- person\n",
      "\n",
      "vadditionalcontactinfo -> (businessentityid - businessentityid) <- personphone\n",
      "\n",
      "vadditionalcontactinfo -> (businessentityid - businessentityid) <- person\n",
      "\n",
      "vadditionalcontactinfo -> (businessentityid - businessentityid) <- password\n",
      "\n",
      "vadditionalcontactinfo -> (businessentityid - personid) <- businessentitycontact\n",
      "\n",
      "vadditionalcontactinfo -> (businessentityid - businessentityid) <- businessentity\n",
      "\n",
      "vadditionalcontactinfo -> (businessentityid - emailaddressid) <- emailaddress\n",
      "\n",
      "vadditionalcontactinfo -> (businessentityid - businessentityid) <- emailaddress\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%local \n",
    "\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "daend = int(datetime.timestamp(datetime.now())*1000)\n",
    "\n",
    "a = datetime(2022, 11, 23, 12, 12, 12)\n",
    "dastart = int(datetime.timestamp(a)*1000)\n",
    "\n",
    "print(dastart)\n",
    "print(daend)\n",
    "\n",
    "graph = GraphStorage(\"bolt://localhost:7687\", \"neo4j\", \"password\", env, False)\n",
    "es = Elasticsearch()\n",
    "\n",
    "res = search_dataset_for_topic(index_content[env], \"Eminhizer\")\n",
    "print(res)\n",
    "\n",
    "# find related datasets\n",
    "\n",
    "for r in res:\n",
    "    related_nodes_by_signature_to_2_grade = graph.find_related_tables_by_signature_to_2_grade(r['dataset'], '0.8', dastart, daend)\n",
    "    \n",
    "    print(related_nodes_by_signature_to_2_grade)\n",
    "    for node in related_nodes_by_signature_to_2_grade: \n",
    "        print(str(node[0]['idx']) + ' - ')\n",
    "        print(str(node[1]['idx']) + ' - ')\n",
    "        print(str(node[2]['idx']) + '\\n')\n",
    "        \n",
    "for r in res:\n",
    "    related_nodes_by_content_to_1_grad = graph.find_related_tables_by_content_to_1_grad(r['attribute'], '0.5', dastart, daend)\n",
    "\n",
    "    for node in related_nodes_by_content_to_1_grad: \n",
    "        print(str(node[0]['dataset']) + ' -> (' + str(node[1]['attribute']) + ' - ' + str(node[2]['attribute']) + ') <- ' + str(node[3]['dataset']) + '\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d2749e8-d721-42e8-ba44-cd4fb8b78229",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%local \n",
    "\n",
    "graph = GraphStorage(\"bolt://localhost:7687\", \"neo4j\", \"password\", env, False)\n",
    "es = Elasticsearch()\n",
    "\n",
    "res = search_dataset_for_topic(index_content[env], \"Eminhizer\")\n",
    "\n",
    "# find related datasets\n",
    "\n",
    "related_datasets = {\n",
    "    'related_nodes_by_signature_to_2_grade': [],\n",
    "    'related_nodes_by_content_to_1_grad': []\n",
    "}\n",
    "\n",
    "for r in res:\n",
    "    related_datasets['related_nodes_by_signature_to_2_grade'].append(graph.find_related_tables_by_signature_to_2_grade(r['dataset'], '0.8'))\n",
    "\n",
    "for r in res:\n",
    "    related_datasets['related_nodes_by_content_to_1_grad'].append(graph.find_related_tables_by_content_to_1_grad(r['id'], '0.5'))\n",
    "    \n",
    "\n",
    "for node in related_datasets['related_nodes_by_signature_to_2_grade']:\n",
    "    print(node)\n",
    "    if (node):\n",
    "        print(str(node[0]['name']) + ' - ')\n",
    "        print(str(node[1]['name']) + ' - ')\n",
    "        print(str(node[2]['name']) + '\\n')\\\n",
    "\n",
    "for node in related_datasets['related_nodes_by_content_to_1_grad']:\n",
    "    print(node)\n",
    "    if node:\n",
    "        a = node[0]\n",
    "        print(str(a['name']) + ' -> (' + str(node[1]['name']) + ' - ' + str(node[2]['name']) + ') <- ' + str(node[3]['name']) + '\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "86019510-f176-4bcd-9f7e-9aae7a0edd41",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%local \n",
    "\n",
    "arr = np.array([ 42114592,  127266168,  215678469,   17266034,   34121180,  942441018,\n",
    "  227629861,  772424727,  258055101,  289724735,  117062477,  555008702,\n",
    "   29604049,  840321879,  378698042, 1209184744,  243903852,   64306427,\n",
    "  418573613,   50885453,  443551390,  238376316,  629454598,  646390910,\n",
    "  258707738,  287206176,   86790830,  230629753,  353312205,   72417695,\n",
    "  113209494,  247807691,  494356438,  598711389,  163905412,   92705481,\n",
    "  167455200,  120333262,  690658470,  503937613,   13774441,  138757320,\n",
    "  429215096,  166965839,  604024314,  151611451,  482383227,   80185077,\n",
    "  132702729,  637863885,  809374076,   15601261,  231429825,  508796949,\n",
    "   93190304,  246310319,  252884464,   76719615,   27566880,  600163519,\n",
    "    9727857,   11786542,  156050497,  303860790,  228751560,  912519144,\n",
    "    9156249,  369895830,  727826405,  416426394, 1081480011,   66120612,\n",
    "  135376155,  210143265, 1031356013,   90284798, 1016601632,   13573364,\n",
    "   68970483,  161433822,  542646600,   96345609,  685539749,   84541516,\n",
    "   86327512,  187842457,  132296212,  857191589,   20737788,  590826688,\n",
    "  444073826,  192175731,  212992765,  114574843,  171782812,   70248969,\n",
    "  202706043,  702713690,  338155350,  342468131,  542793283,  261167155,\n",
    "  378396244,  211528111,  231333263,  103754312,  129317962,   55448546,\n",
    " 1041271092,  594880347,  931295081,  225781925,   83509603,  776091994,\n",
    "  298124272,  467369673,    8909661,   60688953,  115820188,   54821235,\n",
    "  168100490,   33183095,  712469771,  323850472,  414195156,  550484318,\n",
    "  388634331,   42270916], dtype=np.int64)\n",
    "\n",
    "mt = MinHash(num_perm=128)\n",
    "mt.hashvalues = arr\n",
    "mt.seed = 1\n",
    "\n",
    "lsh['0.7'].query(mt)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "python",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
